{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Enter State Farm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T09:14:26.549197Z",
     "start_time": "2018-05-11T09:14:24.944939Z"
    }
   },
   "outputs": [],
   "source": [
    "from fastai.conv_learner import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T09:14:26.556744Z",
     "start_time": "2018-05-11T09:14:26.551113Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# from __future__ import print_function, division\n",
    "path = \"data/state/\"\n",
    "path = \"data/state/sample/\"\n",
    "# import utils; reload(utils)\n",
    "# from utils import *\n",
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T09:14:26.561275Z",
     "start_time": "2018-05-11T09:14:26.558566Z"
    }
   },
   "outputs": [],
   "source": [
    "bs=64\n",
    "sz=224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T06:40:19.266291Z",
     "start_time": "2018-05-10T06:40:19.263500Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH = Path('data/state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T06:41:28.888487Z",
     "start_time": "2018-05-10T06:40:55.301930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:10<00:00,  2.34it/s]\n",
      "100%|██████████| 16/16 [00:06<00:00,  2.30it/s]\n"
     ]
    }
   ],
   "source": [
    "arch=vgg16 # With BatchNorm\n",
    "data = ImageClassifierData.from_paths(path, bs=bs, tfms=tfms_from_model(arch, sz, max_zoom=0.1))\n",
    "learn = ConvLearner.pretrained(arch, data, precompute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T06:41:28.893421Z",
     "start_time": "2018-05-10T06:41:28.890746Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.crit = F.nll_loss\n",
    "learn.metrics = [accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T06:41:29.346617Z",
     "start_time": "2018-05-10T06:41:28.895311Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e651e1c8759246bcaa7329bf0701ece5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                \n",
      "    0      4.250473   209.11381  0.137     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T06:41:29.534479Z",
     "start_time": "2018-05-10T06:41:29.348309Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-run sample experiments on full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should find that everything that worked on the sample (see statefarm-sample.ipynb), works on the full dataset too. Only better! Because now we have more data. So let's see how they go - the models in this section are exact copies of the sample notebook models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Single conv layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def conv1(batches):\n",
    "    model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "            Convolution2D(32,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Convolution2D(64,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Flatten(),\n",
    "            Dense(200, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit_generator(batches, batches.nb_sample, nb_epoch=2, validation_data=val_batches, \n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "    model.optimizer.lr = 0.001\n",
    "    model.fit_generator(batches, batches.nb_sample, nb_epoch=4, validation_data=val_batches, \n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "18946/18946 [==============================] - 114s - loss: 0.2273 - acc: 0.9405 - val_loss: 2.4946 - val_acc: 0.2826\n",
      "Epoch 2/2\n",
      "18946/18946 [==============================] - 114s - loss: 0.0120 - acc: 0.9990 - val_loss: 1.5872 - val_acc: 0.5253\n",
      "Epoch 1/4\n",
      "18946/18946 [==============================] - 114s - loss: 0.0093 - acc: 0.9992 - val_loss: 1.4836 - val_acc: 0.5825\n",
      "Epoch 2/4\n",
      "18946/18946 [==============================] - 114s - loss: 0.0032 - acc: 1.0000 - val_loss: 1.3142 - val_acc: 0.6162\n",
      "Epoch 3/4\n",
      "18946/18946 [==============================] - 114s - loss: 0.0035 - acc: 0.9996 - val_loss: 1.5061 - val_acc: 0.5771\n",
      "Epoch 4/4\n",
      "18946/18946 [==============================] - 114s - loss: 0.0036 - acc: 0.9997 - val_loss: 1.4528 - val_acc: 0.5808\n"
     ]
    }
   ],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Interestingly, with no regularization or augmentation we're getting some reasonable results from our simple convolutional model. So with augmentation, we hopefully will see some very good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18946 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "batches = get_batches(path+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "18946/18946 [==============================] - 114s - loss: 1.2804 - acc: 0.5891 - val_loss: 2.0614 - val_acc: 0.3407\n",
      "Epoch 2/2\n",
      "18946/18946 [==============================] - 114s - loss: 0.6716 - acc: 0.7916 - val_loss: 1.3377 - val_acc: 0.6208\n",
      "Epoch 1/4\n",
      "18946/18946 [==============================] - 115s - loss: 0.4787 - acc: 0.8594 - val_loss: 1.2230 - val_acc: 0.6228\n",
      "Epoch 2/4\n",
      "18946/18946 [==============================] - 114s - loss: 0.3724 - acc: 0.8931 - val_loss: 1.3030 - val_acc: 0.6282\n",
      "Epoch 3/4\n",
      "18946/18946 [==============================] - 114s - loss: 0.3086 - acc: 0.9162 - val_loss: 1.1986 - val_acc: 0.7119\n",
      "Epoch 4/4\n",
      "18946/18946 [==============================] - 114s - loss: 0.2612 - acc: 0.9283 - val_loss: 1.4794 - val_acc: 0.5799\n"
     ]
    }
   ],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "18946/18946 [==============================] - 114s - loss: 0.2391 - acc: 0.9361 - val_loss: 1.2511 - val_acc: 0.6886\n",
      "Epoch 2/15\n",
      "18946/18946 [==============================] - 114s - loss: 0.2075 - acc: 0.9430 - val_loss: 1.1327 - val_acc: 0.7294\n",
      "Epoch 3/15\n",
      "18946/18946 [==============================] - 114s - loss: 0.1800 - acc: 0.9529 - val_loss: 1.1099 - val_acc: 0.7294\n",
      "Epoch 4/15\n",
      "18946/18946 [==============================] - 114s - loss: 0.1675 - acc: 0.9557 - val_loss: 1.0660 - val_acc: 0.7363\n",
      "Epoch 5/15\n",
      "18946/18946 [==============================] - 114s - loss: 0.1432 - acc: 0.9625 - val_loss: 1.1585 - val_acc: 0.7073\n",
      "Epoch 6/15\n",
      "18946/18946 [==============================] - 114s - loss: 0.1358 - acc: 0.9627 - val_loss: 1.1389 - val_acc: 0.6947\n",
      "Epoch 7/15\n",
      "18946/18946 [==============================] - 114s - loss: 0.1283 - acc: 0.9665 - val_loss: 1.1329 - val_acc: 0.7369\n",
      "Epoch 8/15\n",
      "18946/18946 [==============================] - 114s - loss: 0.1180 - acc: 0.9686 - val_loss: 1.1817 - val_acc: 0.7194\n",
      "Epoch 9/15\n",
      "18946/18946 [==============================] - 114s - loss: 0.1137 - acc: 0.9704 - val_loss: 1.0923 - val_acc: 0.7142\n",
      "Epoch 10/15\n",
      "18946/18946 [==============================] - 114s - loss: 0.1076 - acc: 0.9720 - val_loss: 1.0983 - val_acc: 0.7358\n",
      "Epoch 11/15\n",
      "18946/18946 [==============================] - 114s - loss: 0.1032 - acc: 0.9736 - val_loss: 1.0206 - val_acc: 0.7458\n",
      "Epoch 12/15\n",
      "18946/18946 [==============================] - 114s - loss: 0.0956 - acc: 0.9740 - val_loss: 0.9039 - val_acc: 0.7809\n",
      "Epoch 13/15\n",
      "18946/18946 [==============================] - 114s - loss: 0.0962 - acc: 0.9740 - val_loss: 1.3386 - val_acc: 0.6587\n",
      "Epoch 14/15\n",
      "18946/18946 [==============================] - 114s - loss: 0.0892 - acc: 0.9777 - val_loss: 1.1150 - val_acc: 0.7470\n",
      "Epoch 15/15\n",
      "18946/18946 [==============================] - 114s - loss: 0.0886 - acc: 0.9773 - val_loss: 1.9190 - val_acc: 0.5802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3b6b66f610>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=15, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I'm shocked by *how* good these results are! We're regularly seeing 75-80% accuracy on the validation set, which puts us into the top third or better of the competition. With such a simple model and no dropout or semi-supervised learning, this really speaks to the power of this approach to data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Four conv/pooling pairs + dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Unfortunately, the results are still very unstable - the validation accuracy jumps from epoch to epoch. Perhaps a deeper model with some dropout would help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18946 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "batches = get_batches(path+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "        Convolution2D(32,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(64,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(128,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=10e-5), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "18946/18946 [==============================] - 159s - loss: 2.6578 - acc: 0.2492 - val_loss: 1.8681 - val_acc: 0.3844\n",
      "Epoch 2/2\n",
      "18946/18946 [==============================] - 158s - loss: 1.8098 - acc: 0.4334 - val_loss: 1.3152 - val_acc: 0.5670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f227f103ad0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=2, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18946/18946 [==============================] - 159s - loss: 1.4232 - acc: 0.5405 - val_loss: 1.0877 - val_acc: 0.6452\n",
      "Epoch 2/10\n",
      "18946/18946 [==============================] - 159s - loss: 1.1155 - acc: 0.6346 - val_loss: 1.2730 - val_acc: 0.6878\n",
      "Epoch 3/10\n",
      "18946/18946 [==============================] - 159s - loss: 0.9043 - acc: 0.7025 - val_loss: 1.1393 - val_acc: 0.6354\n",
      "Epoch 4/10\n",
      "18946/18946 [==============================] - 159s - loss: 0.7444 - acc: 0.7529 - val_loss: 1.1037 - val_acc: 0.7087\n",
      "Epoch 5/10\n",
      "18946/18946 [==============================] - 159s - loss: 0.6299 - acc: 0.7955 - val_loss: 0.9123 - val_acc: 0.7455\n",
      "Epoch 6/10\n",
      "18946/18946 [==============================] - 159s - loss: 0.5220 - acc: 0.8275 - val_loss: 1.0418 - val_acc: 0.7484\n",
      "Epoch 7/10\n",
      "18946/18946 [==============================] - 159s - loss: 0.4686 - acc: 0.8495 - val_loss: 1.2907 - val_acc: 0.6599\n",
      "Epoch 8/10\n",
      "18946/18946 [==============================] - 159s - loss: 0.4190 - acc: 0.8653 - val_loss: 1.1321 - val_acc: 0.6906\n",
      "Epoch 9/10\n",
      "18946/18946 [==============================] - 159s - loss: 0.3735 - acc: 0.8802 - val_loss: 1.1235 - val_acc: 0.7458\n",
      "Epoch 10/10\n",
      "18946/18946 [==============================] - 159s - loss: 0.3226 - acc: 0.8969 - val_loss: 1.2040 - val_acc: 0.7343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f227f104d10>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=10, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18946/18946 [==============================] - 159s - loss: 0.3183 - acc: 0.8976 - val_loss: 1.0359 - val_acc: 0.7688\n",
      "Epoch 2/10\n",
      "18946/18946 [==============================] - 158s - loss: 0.2788 - acc: 0.9109 - val_loss: 1.5806 - val_acc: 0.6705\n",
      "Epoch 3/10\n",
      "18946/18946 [==============================] - 158s - loss: 0.2810 - acc: 0.9124 - val_loss: 0.9836 - val_acc: 0.7887\n",
      "Epoch 4/10\n",
      "18946/18946 [==============================] - 158s - loss: 0.2403 - acc: 0.9244 - val_loss: 1.1832 - val_acc: 0.7493\n",
      "Epoch 5/10\n",
      "18946/18946 [==============================] - 159s - loss: 0.2195 - acc: 0.9303 - val_loss: 1.1524 - val_acc: 0.7510\n",
      "Epoch 6/10\n",
      "18946/18946 [==============================] - 159s - loss: 0.2085 - acc: 0.9359 - val_loss: 1.2245 - val_acc: 0.7415\n",
      "Epoch 7/10\n",
      "18946/18946 [==============================] - 158s - loss: 0.1961 - acc: 0.9399 - val_loss: 1.1232 - val_acc: 0.7654\n",
      "Epoch 8/10\n",
      "18946/18946 [==============================] - 158s - loss: 0.1851 - acc: 0.9416 - val_loss: 1.0956 - val_acc: 0.6892\n",
      "Epoch 9/10\n",
      "18946/18946 [==============================] - 158s - loss: 0.1798 - acc: 0.9451 - val_loss: 1.0586 - val_acc: 0.7740\n",
      "Epoch 10/10\n",
      "18946/18946 [==============================] - 159s - loss: 0.1669 - acc: 0.9471 - val_loss: 1.4633 - val_acc: 0.6656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f227f104ed0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=10, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is looking quite a bit better - the accuracy is similar, but the stability is higher. There's still some way to go however..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imagenet conv features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have so little data, and it is similar to imagenet images (full color photos), using pre-trained VGG weights is likely to be helpful - in fact it seems likely that we won't need to fine-tune the convolutional layer weights much, if at all. So we can pre-compute the output of the last convolutional layer, as we did in lesson 3 when we experimented with dropout. (However this means that we can't use full data augmentation, since we can't pre-compute something that changes every image.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = Vgg16()\n",
    "model=vgg.model\n",
    "last_conv_idx = [i for i,l in enumerate(model.layers) if type(l) is Convolution2D][-1]\n",
    "conv_layers = model.layers[:last_conv_idx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batches shuffle must be set to False when pre-computing features\n",
    "batches = get_batches(path+'train', batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18946 images belonging to 10 classes.\n",
      "Found 3478 images belonging to 10 classes.\n",
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, \n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_feat = conv_model.predict_generator(batches, batches.nb_sample)\n",
    "conv_val_feat = conv_model.predict_generator(val_batches, val_batches.nb_sample)\n",
    "conv_test_feat = conv_model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path+'results/conv_val_feat.dat', conv_val_feat)\n",
    "save_array(path+'results/conv_test_feat.dat', conv_test_feat)\n",
    "save_array(path+'results/conv_feat.dat', conv_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3478, 512, 14, 14)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_feat = load_array(path+'results/conv_feat.dat')\n",
    "conv_val_feat = load_array(path+'results/conv_val_feat.dat')\n",
    "conv_val_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batchnorm dense layers on pretrained conv layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've pre-computed the output of the last convolutional layer, we need to create a network that takes that as input, and predicts our 10 classes. Let's try using a simplified version of VGG's dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bn_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dropout(p/2),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p/2),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(10, activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18946 samples, validate on 3478 samples\n",
      "Epoch 1/1\n",
      "18946/18946 [==============================] - 3s - loss: 1.5894 - acc: 0.5625 - val_loss: 0.7031 - val_acc: 0.7522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdfd921a690>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(conv_feat, trn_labels, batch_size=batch_size, nb_epoch=1, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18946 samples, validate on 3478 samples\n",
      "Epoch 1/2\n",
      "18946/18946 [==============================] - 3s - loss: 0.2870 - acc: 0.9109 - val_loss: 0.7728 - val_acc: 0.7683\n",
      "Epoch 2/2\n",
      "18946/18946 [==============================] - 3s - loss: 0.1422 - acc: 0.9594 - val_loss: 0.7576 - val_acc: 0.7936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdfd921a8d0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(conv_feat, trn_labels, batch_size=batch_size, nb_epoch=2, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.save_weights(path+'models/conv8.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good! Let's try pre-computing 5 epochs worth of augmented data, so we can experiment with combining dropout and augmentation on the pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-computed data augmentation + dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use our usual data augmentation parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18946 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "da_batches = get_batches(path+'train', gen_t, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use those to create a dataset of convolutional features 5x bigger than the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "da_conv_feat = conv_model.predict_generator(da_batches, da_batches.nb_sample*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array(path+'results/da_conv_feat2.dat', da_conv_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "da_conv_feat = load_array(path+'results/da_conv_feat2.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include the real training data as well in its non-augmented form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "da_conv_feat = np.concatenate([da_conv_feat, conv_feat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've now got a dataset 6x bigger than before, we'll need to copy our labels 6 times too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "da_trn_labels = np.concatenate([trn_labels]*6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on some experiments the previous model works well, with bigger dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bn_da_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dropout(p),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(10, activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model = Sequential(get_bn_da_layers(p))\n",
    "bn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model as usual, with pre-computed augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 113676 samples, validate on 3478 samples\n",
      "Epoch 1/1\n",
      "113676/113676 [==============================] - 16s - loss: 1.5848 - acc: 0.5068 - val_loss: 0.6340 - val_acc: 0.8131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdd886a7c90>"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(da_conv_feat, da_trn_labels, batch_size=batch_size, nb_epoch=1, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 113676 samples, validate on 3478 samples\n",
      "Epoch 1/4\n",
      "113676/113676 [==============================] - 16s - loss: 0.6652 - acc: 0.7785 - val_loss: 0.6343 - val_acc: 0.8082\n",
      "Epoch 2/4\n",
      "113676/113676 [==============================] - 16s - loss: 0.5247 - acc: 0.8318 - val_loss: 0.6951 - val_acc: 0.8085\n",
      "Epoch 3/4\n",
      "113676/113676 [==============================] - 16s - loss: 0.4553 - acc: 0.8544 - val_loss: 0.6067 - val_acc: 0.8189\n",
      "Epoch 4/4\n",
      "113676/113676 [==============================] - 16s - loss: 0.4127 - acc: 0.8686 - val_loss: 0.7701 - val_acc: 0.7915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdd88642490>"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(da_conv_feat, da_trn_labels, batch_size=batch_size, nb_epoch=4, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 113676 samples, validate on 3478 samples\n",
      "Epoch 1/4\n",
      "113676/113676 [==============================] - 16s - loss: 0.3837 - acc: 0.8775 - val_loss: 0.6904 - val_acc: 0.8197\n",
      "Epoch 2/4\n",
      "113676/113676 [==============================] - 16s - loss: 0.3576 - acc: 0.8872 - val_loss: 0.6593 - val_acc: 0.8209\n",
      "Epoch 3/4\n",
      "113676/113676 [==============================] - 16s - loss: 0.3384 - acc: 0.8939 - val_loss: 0.7057 - val_acc: 0.8085\n",
      "Epoch 4/4\n",
      "113676/113676 [==============================] - 16s - loss: 0.3254 - acc: 0.8977 - val_loss: 0.6867 - val_acc: 0.8128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdd88642710>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(da_conv_feat, da_trn_labels, batch_size=batch_size, nb_epoch=4, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good - let's save those weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model.save_weights(path+'models/da_conv8_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Pseudo labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We're going to try using a combination of [pseudo labeling](http://deeplearning.net/wp-content/uploads/2013/03/pseudo_label_final.pdf) and [knowledge distillation](https://arxiv.org/abs/1503.02531) to allow us to use unlabeled data (i.e. do semi-supervised learning). For our initial experiment we'll use the validation set as the unlabeled data, so that we can see that it is working without using the test set. At a later date we'll try using the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To do this, we simply calculate the predictions of our model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val_pseudo = bn_model.predict(conv_val_feat, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "...concatenate them with our training labels..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "comb_pseudo = np.concatenate([da_trn_labels, val_pseudo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "comb_feat = np.concatenate([da_conv_feat, conv_val_feat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "...and fine-tune our model using that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bn_model.load_weights(path+'models/da_conv8_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 117154 samples, validate on 3478 samples\n",
      "Epoch 1/1\n",
      "117154/117154 [==============================] - 17s - loss: 0.3412 - acc: 0.8948 - val_loss: 0.7653 - val_acc: 0.8191\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdd88642f50>"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(comb_feat, comb_pseudo, batch_size=batch_size, nb_epoch=1, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 117154 samples, validate on 3478 samples\n",
      "Epoch 1/4\n",
      "117154/117154 [==============================] - 17s - loss: 0.3237 - acc: 0.9008 - val_loss: 0.7536 - val_acc: 0.8229\n",
      "Epoch 2/4\n",
      "117154/117154 [==============================] - 17s - loss: 0.3076 - acc: 0.9050 - val_loss: 0.7572 - val_acc: 0.8235\n",
      "Epoch 3/4\n",
      "117154/117154 [==============================] - 17s - loss: 0.2984 - acc: 0.9085 - val_loss: 0.7852 - val_acc: 0.8269\n",
      "Epoch 4/4\n",
      "117154/117154 [==============================] - 17s - loss: 0.2902 - acc: 0.9117 - val_loss: 0.7630 - val_acc: 0.8263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdd89bdd210>"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(comb_feat, comb_pseudo, batch_size=batch_size, nb_epoch=4, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr=0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 117154 samples, validate on 3478 samples\n",
      "Epoch 1/4\n",
      "117154/117154 [==============================] - 17s - loss: 0.2837 - acc: 0.9134 - val_loss: 0.7901 - val_acc: 0.8200\n",
      "Epoch 2/4\n",
      "117154/117154 [==============================] - 17s - loss: 0.2760 - acc: 0.9155 - val_loss: 0.7648 - val_acc: 0.8275\n",
      "Epoch 3/4\n",
      "117154/117154 [==============================] - 17s - loss: 0.2723 - acc: 0.9183 - val_loss: 0.7382 - val_acc: 0.8358\n",
      "Epoch 4/4\n",
      "117154/117154 [==============================] - 17s - loss: 0.2657 - acc: 0.9191 - val_loss: 0.7227 - val_acc: 0.8329\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdd89bb2890>"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(comb_feat, comb_pseudo, batch_size=batch_size, nb_epoch=4, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "That's a distinct improvement - even although the validation set isn't very big. This looks encouraging for when we try this on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bn_model.save_weights(path+'models/bn-ps8.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll find a good clipping amount using the validation set, prior to submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_clip(arr, mx): return np.clip(arr, (1-mx)/9, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.6726388006592667)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.metrics.categorical_crossentropy(val_labels, do_clip(val_preds, 0.93)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_test_feat = load_array(path+'results/conv_test_feat.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preds = bn_model.predict(conv_test_feat, batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm = do_clip(preds,0.93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm_name = path+'results/subm.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted(batches.class_indices, key=batches.class_indices.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>c0</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "      <th>c9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_68347.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.093739</td>\n",
       "      <td>0.815874</td>\n",
       "      <td>0.079049</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_55725.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_92799.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.017918</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.009022</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_72170.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.363869</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.200521</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.425176</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_59154.jpg</td>\n",
       "      <td>0.695756</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.047384</td>\n",
       "      <td>0.249183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             img        c0        c1        c2        c3        c4        c5  \\\n",
       "0  img_68347.jpg  0.007778  0.007778  0.007778  0.007778  0.007778  0.007778   \n",
       "1  img_55725.jpg  0.007778  0.007778  0.007778  0.007778  0.007778  0.007778   \n",
       "2  img_92799.jpg  0.007778  0.930000  0.017918  0.007778  0.007778  0.007778   \n",
       "3  img_72170.jpg  0.007778  0.007778  0.363869  0.007778  0.007778  0.007778   \n",
       "4  img_59154.jpg  0.695756  0.007778  0.007778  0.007778  0.007778  0.007778   \n",
       "\n",
       "         c6        c7        c8        c9  \n",
       "0  0.093739  0.815874  0.079049  0.007778  \n",
       "1  0.007778  0.007778  0.930000  0.007778  \n",
       "2  0.009022  0.007778  0.007778  0.007778  \n",
       "3  0.200521  0.007778  0.425176  0.007778  \n",
       "4  0.007778  0.007778  0.047384  0.249183  "
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame(subm, columns=classes)\n",
    "submission.insert(0, 'img', [a[4:] for a in test_filenames])\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(subm_name, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='data/state/results/subm.gz' target='_blank'>data/state/results/subm.gz</a><br>"
      ],
      "text/plain": [
       "/data/jhoward/fast-image/nbs/data/state/results/subm.gz"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink(subm_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets 0.534 on the leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## The \"things that didn't really work\" section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can safely ignore everything from here on, because they didn't really help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Finetune some conv layers too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for l in get_bn_layers(p): conv_model.add(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for l1,l2 in zip(bn_model.layers, conv_model.layers[last_conv_idx+1:]):\n",
    "    l2.set_weights(l1.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for l in conv_model.layers: l.trainable =False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for l in conv_model.layers[last_conv_idx+1:]: l.trainable =True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "comb = np.concatenate([trn, val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=8, height_shift_range=0.04, \n",
    "                shear_range=0.03, channel_shift_range=10, width_shift_range=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "X (images tensor) and y (labels) should have the same length. Found: X.shape = (22424, 3, 224, 224), y.shape = (98208, 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-8e21fbf7f6e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomb_pseudo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow\u001b[1;34m(self, X, y, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format)\u001b[0m\n\u001b[0;32m    274\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[0mdim_ordering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim_ordering\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m             save_to_dir=save_to_dir, save_prefix=save_prefix, save_format=save_format)\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m     def flow_from_directory(self, directory,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, X, y, image_data_generator, batch_size, shuffle, seed, dim_ordering, save_to_dir, save_prefix, save_format)\u001b[0m\n\u001b[0;32m    473\u001b[0m             raise Exception('X (images tensor) and y (labels) '\n\u001b[0;32m    474\u001b[0m                             \u001b[1;34m'should have the same length. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m                             'Found: X.shape = %s, y.shape = %s' % (np.asarray(X).shape, np.asarray(y).shape))\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdim_ordering\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mdim_ordering\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_dim_ordering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: X (images tensor) and y (labels) should have the same length. Found: X.shape = (22424, 3, 224, 224), y.shape = (98208, 10)"
     ]
    }
   ],
   "source": [
    "batches = gen_t.flow(comb, comb_pseudo, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3478 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "val_batches = get_batches(path+'valid', batch_size=batch_size*2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv_model.compile(Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "22400/22424 [============================>.] - ETA: 0s - loss: 0.4348 - acc: 0.9200"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Error allocating 1644167168 bytes of device memory (CNMEM_STATUS_OUT_OF_MEMORY).\nApply node that caused the error: GpuAllocEmpty(Shape_i{0}.0, Shape_i{0}.0, Elemwise{Composite{(((i0 - i1) // i2) + i2)}}[(0, 1)].0, Elemwise{Composite{(((i0 - i1) // i2) + i2)}}[(0, 1)].0)\nToposort index: 157\nInputs types: [TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar)]\nInputs shapes: [(), (), (), ()]\nInputs strides: [(), (), (), ()]\nInputs values: [array(128), array(64), array(224), array(224)]\nOutputs clients: [[GpuDnnConv{algo='small', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty.0, GpuDnnConvDesc{border_mode='valid', subsample=(1, 1), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-178-50c2f05dc6a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m conv_model.fit_generator(batches, batches.N, nb_epoch=1, validation_data=val_batches, \n\u001b[1;32m----> 2\u001b[1;33m                  nb_val_samples=val_batches.N)\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, **kwargs)\u001b[0m\n\u001b[0;32m    872\u001b[0m                                         \u001b[0mmax_q_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 874\u001b[1;33m                                         pickle_safe=pickle_safe)\n\u001b[0m\u001b[0;32m    875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_q_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_worker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_safe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[0;32m   1469\u001b[0m                         val_outs = self.evaluate_generator(validation_data,\n\u001b[0;32m   1470\u001b[0m                                                            \u001b[0mnb_val_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1471\u001b[1;33m                                                            max_q_size=max_q_size)\n\u001b[0m\u001b[0;32m   1472\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1473\u001b[0m                         \u001b[1;31m# no need for try/except because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(self, generator, val_samples, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[0;32m   1552\u001b[0m                                 'or (x, y). Found: ' + str(generator_output))\n\u001b[0;32m   1553\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1554\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1555\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1556\u001b[0m                 \u001b[0m_stop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtest_on_batch\u001b[1;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[0;32m   1257\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1259\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1260\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1261\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    715\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 717\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    869\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[0;32m    872\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                 \u001b[1;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Error allocating 1644167168 bytes of device memory (CNMEM_STATUS_OUT_OF_MEMORY).\nApply node that caused the error: GpuAllocEmpty(Shape_i{0}.0, Shape_i{0}.0, Elemwise{Composite{(((i0 - i1) // i2) + i2)}}[(0, 1)].0, Elemwise{Composite{(((i0 - i1) // i2) + i2)}}[(0, 1)].0)\nToposort index: 157\nInputs types: [TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar)]\nInputs shapes: [(), (), (), ()]\nInputs strides: [(), (), (), ()]\nInputs values: [array(128), array(64), array(224), array(224)]\nOutputs clients: [[GpuDnnConv{algo='small', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty.0, GpuDnnConvDesc{border_mode='valid', subsample=(1, 1), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "conv_model.fit_generator(batches, batches.N, nb_epoch=1, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv_model.optimizer.lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conv_model.fit_generator(batches, batches.N, nb_epoch=3, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for l in conv_model.layers[16:]: l.trainable =True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv_model.optimizer.lr = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conv_model.fit_generator(batches, batches.N, nb_epoch=8, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv_model.save_weights(path+'models/conv8_ps.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv_model.load_weights(path+'models/conv8_da.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val_pseudo = conv_model.predict(val, batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_array(path+'models/pseudo8_da.dat', val_pseudo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_44733.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_72999.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_25094.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_69092.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_92629.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject classname            img\n",
       "0    p002        c0  img_44733.jpg\n",
       "1    p002        c0  img_72999.jpg\n",
       "2    p002        c0  img_25094.jpg\n",
       "3    p002        c0  img_69092.jpg\n",
       "4    p002        c0  img_92629.jpg"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drivers_ds = pd.read_csv(path+'driver_imgs_list.csv')\n",
    "drivers_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img2driver = drivers_ds.set_index('img')['subject'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "driver2imgs = {k: g[\"img\"].tolist() \n",
    "               for k,g in drivers_ds[['subject', 'img']].groupby(\"subject\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_idx(driver_list):\n",
    "    return [i for i,f in enumerate(filenames) if img2driver[f[3:]] in driver_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "drivers = driver2imgs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rnd_drivers = np.random.permutation(drivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ds1 = rnd_drivers[:len(rnd_drivers)//2]\n",
    "ds2 = rnd_drivers[len(rnd_drivers)//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "models=[fit_conv([d]) for d in drivers]\n",
    "models=[m for m in models if m is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_preds = np.stack([m.predict(conv_test_feat, batch_size=128) for m in models])\n",
    "avg_preds = all_preds.mean(axis=0)\n",
    "avg_preds = avg_preds/np.expand_dims(avg_preds.sum(axis=1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.9753041572894531)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.metrics.categorical_crossentropy(val_labels, np.clip(avg_val_preds,0.01,0.99)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.6949396133422852, dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.metrics.categorical_accuracy(val_labels, np.clip(avg_val_preds,0.01,0.99)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nav_menu": {},
  "nbpresent": {
   "slides": {
    "28b43202-5690-4169-9aca-6b9dabfeb3ec": {
     "id": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "prev": null,
     "regions": {
      "3bba644a-cf4d-4a49-9fbd-e2554428cf9f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f3d3a388-7e2a-4151-9b50-c20498fceacc",
        "part": "whole"
       },
       "id": "3bba644a-cf4d-4a49-9fbd-e2554428cf9f"
      }
     }
    },
    "8104def2-4b68-44a0-8f1b-b03bf3b2a079": {
     "id": "8104def2-4b68-44a0-8f1b-b03bf3b2a079",
     "prev": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "regions": {
      "7dded777-1ddf-4100-99ae-25cf1c15b575": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fe47bd48-3414-4657-92e7-8b8d6cb0df00",
        "part": "whole"
       },
       "id": "7dded777-1ddf-4100-99ae-25cf1c15b575"
      }
     }
    }
   },
   "themes": {}
  },
  "toc": {
   "nav_menu": {
    "height": "148px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
